{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "892c1539-b119-452f-8f09-df3628f63f4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the Medical Knowledge Assistant.\n",
      "Processing your request, please wait...\n",
      "\n",
      "Retrieved context:\n",
      "\n",
      "[0] HADM_ID: 139560\n",
      "Disease_Name: AMI inferior wall, init\n",
      "SUBJECT_ID: 5599\n",
      "Symptom_Keywords: woman known coronary artery disease who reported feeling well sister morning found weak without chest pain shortness breath taken where electrocardiogram showed inferior right sided myocardial infarction catheterization laboratory three vessel total occlusion left circumflex diffusely diseased anterior descending stented times intraaortic balloon pump placed hypotension subsequently went into ventricular fibrillation arrest intubated converted sinus rhythm defibrillation then transferred further management outside received heparin integrilin lidocaine digoxin transiently dopamine asp\n",
      "Standard_Output: Diagnosis: AMI inferior wall, init | Medications: None\n",
      "\n",
      "[1] HADM_ID: 168124\n",
      "Disease_Name: AMI inferior wall, init\n",
      "SUBJECT_ID: 43633\n",
      "Prescriptions: 1/2 NS; Acetaminophen; Albuterol Inhaler; Aspirin EC; Atorvastatin; Atropine Sulfate; Bisacodyl; Clopidogrel; Docusate Sodium (Liquid); Eptifibatide; Heparin; Metoprolol Tartrate; Morphine Sulfate; SW; Senna; Sodium Chloride 0.9%  Flush; traZODONE\n",
      "Symptom_Keywords: substernal chest pain radiating right shoulder down arm\n",
      "Standard_Output: Diagnosis: AMI inferior wall, init | Medications: 1/2 NS; Acetaminophen; Albuterol Inhaler; Aspirin EC; Atorvastatin; Atropine Sulfate; Bisacodyl; Clopidogrel; Docusate Sodium (Liquid); Eptifibatide; Heparin; Metoprolol Tartrate; Morphine Sulfate; SW; Senna; Sodium Chloride 0.9%  Flush; traZODONE\n",
      "\n",
      "[2] HADM_ID: 120410\n",
      "Disease_Name: Crnry athrscl natve vssl\n",
      "SUBJECT_ID: 24693\n",
      "Symptom_Keywords: progressive dyspnea exertion gentleman who recently complained stress test showed inferior wall ischemia ejection fraction inferoseptal motion abnormality subsequently underwent cardiac catheterization revealed three vessel disease came elective surgery\n",
      "Standard_Output: Diagnosis: Crnry athrscl natve vssl | Medications: None\n",
      "\n",
      "Question: Based on the retrieved cases, what symptoms and medications were observed for AMI inferior wall?\n",
      "\n",
      "Answer:\n",
      "For AMI inferior wall, the observed symptoms include:\n",
      "*   weak without chest pain\n",
      "*   shortness breath\n",
      "*   substernal chest pain radiating right shoulder down arm\n",
      "\n",
      "The observed medications include:\n",
      "*   heparin\n",
      "*   integrilin\n",
      "*   lidocaine\n",
      "*   digoxin\n",
      "*   dopamine\n",
      "*   asp\n",
      "*   1/2 NS\n",
      "*   Acetaminophen\n",
      "*   Albuterol Inhaler\n",
      "*   Aspirin EC\n",
      "*   Atorvastatin\n",
      "*   Atropine Sulfate\n",
      "*   Bisacodyl\n",
      "*   Clopidogrel\n",
      "*   Docusate Sodium (Liquid)\n",
      "*   Eptifibatide\n",
      "*   Metoprolol Tartrate\n",
      "*   Morphine Sulfate\n",
      "*   SW\n",
      "*   Senna\n",
      "*   Sodium Chloride 0.9% Flush\n",
      "*   traZODONE\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "print(\"Welcome to the Medical Knowledge Assistant.\")\n",
    "print(\"Processing your request, please wait...\\n\")\n",
    "\n",
    "\n",
    "def split_into_chunks(csv_file: str) -> List[str]:\n",
    "    df = pd.read_csv(csv_file)\n",
    "    chunks = []\n",
    "    for _, row in df.iterrows():\n",
    "        parts = []\n",
    "        for col in df.columns:\n",
    "            \n",
    "            val = row[col]\n",
    "            if pd.isna(val):\n",
    "                continue\n",
    "                \n",
    "            parts.append(f\"{col}: {str(val)}\")\n",
    "        chunks.append(\"\\n\".join(parts))\n",
    "    return chunks\n",
    "\n",
    "chunks = split_into_chunks(\"mimic_dataset_600_keywords.csv\")\n",
    "\n",
    "# for i, chunk in enumerate(chunks[:3]):\n",
    "#     print(f\"[{i}] {chunk}\\n\")\n",
    "\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "embedding_model = SentenceTransformer(\"shibing624/text2vec-base-chinese\")\n",
    "\n",
    "def embed_chunk(chunk: str) -> List[float]:\n",
    "    embedding = embedding_model.encode(chunk, normalize_embeddings=True)\n",
    "    return embedding.tolist()\n",
    "\n",
    "# embedding = embed_chunk(\"test\")\n",
    "# print(len(embedding))\n",
    "\n",
    "\n",
    "\n",
    "embeddings = embedding_model.encode(chunks, normalize_embeddings=True)\n",
    "embeddings = embeddings.tolist()\n",
    "\n",
    "# print(len(embeddings))\n",
    "# print(len(embeddings[0]))\n",
    "\n",
    "\n",
    "import chromadb\n",
    "\n",
    "chromadb_client = chromadb.EphemeralClient()\n",
    "chromadb_collection = chromadb_client.get_or_create_collection(name=\"default3\")\n",
    "\n",
    "def save_embeddings(chunks: List[str], embeddings: List[List[float]]) -> None:\n",
    "    ids = [str(i) for i in range(len(chunks))]\n",
    "    chromadb_collection.add(\n",
    "        documents=chunks,\n",
    "        embeddings=embeddings,\n",
    "        ids=ids\n",
    "    )\n",
    "\n",
    "save_embeddings(chunks, embeddings)\n",
    "\n",
    "\n",
    "def retrieve(query: str, top_k: int) -> List[str]:\n",
    "    query_embedding = embed_chunk(query)\n",
    "    results = chromadb_collection.query(\n",
    "        query_embeddings=[query_embedding],\n",
    "        n_results=top_k\n",
    "    )\n",
    "    return results[\"documents\"][0]\n",
    "\n",
    "\n",
    "# Pick a query\n",
    "\n",
    "def pick_query_from_dataset(chunks: List[str]) -> str:\n",
    "    if not chunks:\n",
    "        return \"sepsis\"\n",
    "    text = chunks[0]\n",
    "    cands = re.findall(r\"[\\u4e00-\\u9fffA-Za-z0-9]{2,}\", text)\n",
    "    if cands:\n",
    "        return cands[0]\n",
    "    return text.splitlines()[0][:20]\n",
    "\n",
    "query = \"Based on the retrieved cases, what symptoms and medications were observed for AMI inferior wall?\"\n",
    "\n",
    "\n",
    "retrieved_chunks = retrieve(query, 10)\n",
    "\n",
    "# for i, chunk in enumerate(retrieved_chunks):\n",
    "#     print(f\"[{i}] {chunk}\\n\")\n",
    "\n",
    "\n",
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "cross_encoder = CrossEncoder(\"cross-encoder/mmarco-mMiniLMv2-L12-H384-v1\")\n",
    "\n",
    "def rerank(query: str, retrieved_chunks: List[str], top_k: int) -> List[str]:\n",
    "    pairs = [(query, chunk) for chunk in retrieved_chunks]\n",
    "    scores = cross_encoder.predict(pairs)\n",
    "\n",
    "    scored_chunks = list(zip(retrieved_chunks, scores))\n",
    "    scored_chunks.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return [chunk for chunk, _ in scored_chunks][:top_k]\n",
    "\n",
    "reranked_chunks = rerank(query, retrieved_chunks, 3)\n",
    "\n",
    "# Print Chunks\n",
    "print(\"Retrieved context:\\n\")\n",
    "for i, chunk in enumerate(reranked_chunks):\n",
    "    print(f\"[{i}] {chunk}\\n\")\n",
    "\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from google import genai\n",
    "\n",
    "load_dotenv()\n",
    "google_client = genai.Client()\n",
    "\n",
    "# API call\n",
    "def generate(query: str, chunks: List[str]) -> str:\n",
    "    prompt = f\"\"\"You are a medical knowledge assistant. Answer the user's question using ONLY the context below.\n",
    "\n",
    "User question:\n",
    "{query}\n",
    "\n",
    "Context:\n",
    "{\"\\n\\n\".join(chunks)}\n",
    "\n",
    "Rules:\n",
    "- Do not invent facts.\n",
    "- If the context is not enough, say you don't know based on the context.\n",
    "- Keep it concise.\n",
    "\"\"\"\n",
    "    response = google_client.models.generate_content(\n",
    "        model=\"gemini-2.5-flash\",\n",
    "        contents=prompt\n",
    "    )\n",
    "    return response.text\n",
    "    \n",
    "print(\"Question:\", query)\n",
    "answer = generate(query, reranked_chunks)\n",
    "print(\"\\nAnswer:\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d03632c-4312-468d-8c45-315c9b525375",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
